{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we build on the previous exercises to prepare a labeled dataset of binary feature vectors, and use it to train a *Random Forest* binary classifier of malware/benign feature vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directoriesWithLabels = [(\"Samples/Benign\",0), (\"Samples/Malware\",1)]\n",
    "listOfSamples = []\n",
    "labels = []\n",
    "for datasetPath, label in directoriesWithLabels:\n",
    "    samples = [f for f in os.listdir(datasetPath)]\n",
    "    for file in samples:\n",
    "        filePath = os.path.join(datasetPath, file)\n",
    "        listOfSamples.append(filePath)\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-Test data split\n",
    "from sklearn.model_selection import train_test_split\n",
    "samples_train, samples_test, labels_train, labels_test = train_test_split(listOfSamples, labels, test_size=0.33, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from nltk import ngrams\n",
    "import numpy as np\n",
    "import pefile\n",
    "\n",
    "def readFile(filePath):\n",
    "    with open(filePath, \"rb\") as binary_file:\n",
    "        data = binary_file.read()\n",
    "    return data\n",
    "\n",
    "def byteSequenceToNgrams(byteSequence, n):\n",
    "    Ngrams = ngrams(byteSequence, n)\n",
    "    return list(Ngrams)\n",
    "    \n",
    "def extractNgramCounts(file, N):\n",
    "    fileByteSequence = readFile(file)\n",
    "    fileNgrams = byteSequenceToNgrams(fileByteSequence, N)\n",
    "    return collections.Counter(fileNgrams)\n",
    "\n",
    "def getNGramFeaturesFromSample(file, K1_most_common_Ngrams_list):\n",
    "    K1 = len(K1_most_common_Ngrams_list)\n",
    "    fv = K1*[0]\n",
    "    fileNgrams = extractNgramCounts(file, N)\n",
    "    for i in range(K1):\n",
    "        fv[i]=fileNgrams[K1_most_common_Ngrams_list[i]]\n",
    "    return fv\n",
    "\n",
    "def preprocessImports(listOfDLLs):\n",
    "    processedListOfDLLs = []\n",
    "    temp = [x.decode().split(\".\")[0].lower() for x in listOfDLLs]\n",
    "    return \" \".join(temp)\n",
    "\n",
    "def getImports(pe):\n",
    "    listOfImports = []\n",
    "    for entry in pe.DIRECTORY_ENTRY_IMPORT:\n",
    "        listOfImports.append(entry.dll)\n",
    "    return preprocessImports(listOfImports)\n",
    "\n",
    "def getSectionNames(pe):\n",
    "    listOfSectionNames = []\n",
    "    for eachSection in pe.sections:\n",
    "        refined_name = eachSection.Name.decode().replace('\\x00','').lower()\n",
    "        listOfSectionNames.append(refined_name)\n",
    "    return \" \".join(listOfSectionNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 2-Grams, \n",
    "# and produce feature vectors based on the frequency method\n",
    "# This may take a few minutes to run\n",
    "N=2\n",
    "totalNgramCount = collections.Counter([])\n",
    "for file in samples_train:\n",
    "    totalNgramCount += extractNgramCounts(file, N)\n",
    "K1 = 100\n",
    "K1_most_common_Ngrams = totalNgramCount.most_common(K1)\n",
    "K1_most_common_Ngrams_list = [x[0] for x in K1_most_common_Ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples/Benign/pmgrant.exe:\n",
      "'Invalid e_lfanew value, probably not a PE file'\n",
      "Samples/Benign/LogCollector.exe:\n",
      "'DOS Header magic not found.'\n",
      "Samples/Benign/urlproxy.exe:\n",
      "'Invalid NT Headers signature. Probably a NE file'\n",
      "Samples/Benign/oisicon.exe:\n",
      "'PE' object has no attribute 'DIRECTORY_ENTRY_IMPORT'\n",
      "Samples/Benign/FixSqlRegistryKey_x64.exe:\n",
      "'PE' object has no attribute 'DIRECTORY_ENTRY_IMPORT'\n",
      "Samples/Benign/Common.DBConnection64.exe:\n",
      "'PE' object has no attribute 'DIRECTORY_ENTRY_IMPORT'\n",
      "Samples/Benign/ldifde.exe:\n",
      "'DOS Header magic not found.'\n",
      "Samples/Benign/lc.exe:\n",
      "'PE' object has no attribute 'DIRECTORY_ENTRY_IMPORT'\n",
      "Samples/Benign/newmail.exe:\n",
      "'Invalid e_lfanew value, probably not a PE file'\n",
      "Samples/Benign/SettingSyncHost.exe:\n",
      "'DOS Header magic not found.'\n",
      "Samples/Benign/RegAsm.exe:\n",
      "'PE' object has no attribute 'DIRECTORY_ENTRY_IMPORT'\n",
      "Samples/Benign/adaminstall.exe:\n",
      "'DOS Header magic not found.'\n",
      "Samples/Benign/sysprep.exe:\n",
      "'DOS Header magic not found.'\n",
      "Samples/Benign/fsynonym.exe:\n",
      "'Invalid e_lfanew value, probably not a PE file'\n",
      "Samples/Benign/pmsort.exe:\n",
      "'Invalid e_lfanew value, probably not a PE file'\n",
      "Samples/Benign/LockAppHost.exe:\n",
      "'DOS Header magic not found.'\n"
     ]
    }
   ],
   "source": [
    "# Extract N-gram features based on the frequency method\n",
    "# Also, extracts some metadata such as DLL imports, \n",
    "# and PE Sections. We will combine these with\n",
    "# our N-gram features to enrich the sample representation.\n",
    "# This will take a few minutes to run.\n",
    "# Some samples will generate errors such as 'not a PE file',\n",
    "# 'DOS header not found', and 'invalid attribute'. These are OK.\n",
    "importsCorpus_train = []\n",
    "numSections_train = []\n",
    "sectionNames_train = []\n",
    "NgramFeaturesList_train = []\n",
    "y_train = []\n",
    "for i in range(len(samples_train)):\n",
    "    file = samples_train[i]\n",
    "    try:\n",
    "        NGramFeatures = getNGramFeaturesFromSample(file, K1_most_common_Ngrams_list)\n",
    "        pe = pefile.PE(file)\n",
    "        imports = getImports(pe)\n",
    "        nSections = len(pe.sections)\n",
    "        secNames = getSectionNames(pe)\n",
    "        importsCorpus_train.append(imports)\n",
    "        numSections_train.append(nSections)\n",
    "        sectionNames_train.append(secNames)\n",
    "        NgramFeaturesList_train.append(NGramFeatures)\n",
    "        y_train.append(labels_train[i])\n",
    "    except Exception as e: \n",
    "        print(file+\":\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following lines, we define a pipeline of sequential transforms (HashingVectorizer and TfidfTransformer) to extract N-gram featurs and construct feature vectors from the DLL imports and Section names extracted for each sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "imports_featurizer = Pipeline([('vect', HashingVectorizer(input='content', ngram_range=(1, 2))),('tfidf', TfidfTransformer(use_idf=True, )),])\n",
    "section_names_featurizer = Pipeline([('vect', HashingVectorizer(input='content', ngram_range=(1, 2))),('tfidf', TfidfTransformer(use_idf=True, )),])\n",
    "importsCorpus_train_transformed = imports_featurizer.fit_transform(importsCorpus_train)\n",
    "sectionNames_train_transformed = section_names_featurizer.fit_transform(sectionNames_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the binary N-gram features with \n",
    "# the DLL imports and section names features to create\n",
    "# vectorized training samples\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "X_train = hstack([NgramFeaturesList_train, importsCorpus_train_transformed,sectionNames_train_transformed, csr_matrix(numSections_train).transpose()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the Random Forest classifier\n",
    "# This may take a few minutes.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(n_estimators=1)\n",
    "clf = clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9936974789915967"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training accuracy\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples/Malware/VirusShare_14f3035781bb698c37ad287483af569e.exe:\n",
      "'utf-8' codec can't decode byte 0x8d in position 0: invalid start byte\n",
      "Samples/Benign/aspnetca.exe:\n",
      "'DOS Header magic not found.'\n",
      "Samples/Benign/InstallUtil.exe:\n",
      "'PE' object has no attribute 'DIRECTORY_ENTRY_IMPORT'\n",
      "Samples/Malware/VirusShare_1a89b7d4fb8ded72e1f8e81ee9352262.exe:\n",
      "'utf-8' codec can't decode byte 0xb1 in position 0: invalid start byte\n",
      "Samples/Malware/VirusShare_7a30183b105b4200fc201925aba4886c.exe:\n",
      "'utf-8' codec can't decode byte 0xb8 in position 0: invalid start byte\n",
      "Samples/Benign/BootExpCfg.exe:\n",
      "'DOS Header magic not found.'\n",
      "Samples/Benign/FixSqlRegistryKey_ia64.exe:\n",
      "'PE' object has no attribute 'DIRECTORY_ENTRY_IMPORT'\n",
      "Samples/Benign/evntwin.exe:\n",
      "'DOS Header magic not found.'\n",
      "Samples/Benign/malias.exe:\n",
      "'Invalid e_lfanew value, probably not a PE file'\n"
     ]
    }
   ],
   "source": [
    "# Generate feature vectors for the test samples\n",
    "# This may take a few minutes\n",
    "importsCorpus_test = []\n",
    "numSections_test = []\n",
    "sectionNames_test = []\n",
    "NgramFeaturesList_test = []\n",
    "y_test = []\n",
    "for i in range(len(samples_test)):\n",
    "    file = samples_test[i]\n",
    "    try:\n",
    "        NGramFeatures = getNGramFeaturesFromSample(file, K1_most_common_Ngrams_list)\n",
    "        pe = pefile.PE(file)\n",
    "        imports = getImports(pe)\n",
    "        nSections = len(pe.sections)\n",
    "        secNames = getSectionNames(pe)\n",
    "        importsCorpus_test.append(imports)\n",
    "        numSections_test.append(nSections)\n",
    "        sectionNames_test.append(secNames)\n",
    "        NgramFeaturesList_test.append(NGramFeatures)\n",
    "        y_test.append(labels_test[i])\n",
    "    except Exception as e: \n",
    "        print(file+\":\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "importsCorpus_test_transformed = imports_featurizer.transform(importsCorpus_test)\n",
    "sectionNames_test_transformed = section_names_featurizer.transform(sectionNames_test)\n",
    "X_test = hstack([NgramFeaturesList_test, importsCorpus_test_transformed,sectionNames_test_transformed, csr_matrix(numSections_test).transpose()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** The training and test accuracies are unusually high. Can you propose a diagnosis for these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- your response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnosis\n",
    "\n",
    "The training and test accuracies were high because mainly of class imblancing. The data nearly has 99% of not malwares because they were obfuscated. the remaining 1% is very tough for the algorithm to identify.\n",
    "\n",
    "how to remedy this\n",
    "\n",
    "1) follow anti class-imbalancing techniques\n",
    "2) Handling Type-1 and Type-II errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
